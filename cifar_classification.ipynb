{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pckQfL8oZhbN"
   },
   "source": [
    "# CIFAR10 Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEt4AJqKVj0v"
   },
   "source": [
    "üéØ **Exercise objectives**\n",
    "\n",
    "* 1Ô∏è‚É£ Implement a CNN to solve a **`10-class classification problem`**\n",
    "\n",
    "* 2Ô∏è‚É£ Enhance the CNN's performance with **`Data Augmentation Techniques`**\n",
    "\n",
    "* 3Ô∏è‚É£ Experiment with **`GPUs to Accelerate Image Processing using Google Colab`**\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "üëè You should now have a better feeling of:\n",
    "* how CNNs works, \n",
    "* and especially how convolutions scan images to detect specific spatial features. \n",
    "\n",
    "üöÄ It's time to play with images that are a bit more complex than the handwritten digits or the triangles/circles. \n",
    "\n",
    "üé® From [Wikipedia](https://en.wikipedia.org/wiki/CIFAR-10) (*click on the link for further information*):\n",
    "\n",
    "> The **`CIFAR-10`** dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.\n",
    "\n",
    "<img src=\"https://people.minesparis.psl.eu/fabien.moutarde/ES_MachineLearning/mini-projets/cifar10_notebook_fichiers/cifar_10.png\">\n",
    "\n",
    "‚≠êÔ∏è This dataset is iconic in the research community as many enhancements for image recognition have been achieved on this dataset. After achieving great performance on this dataset, researchers moved to the more advanced CIFAR-100.\n",
    "\n",
    "From the [University of Toronto](https://www.cs.toronto.edu/~kriz/cifar.html):\n",
    "\n",
    "> This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
    "\n",
    "\n",
    "üî• In this notebook, let's ***implement a CNN to distinguish the 10 categories from the CIFAR-10 dataset***.\n",
    "\n",
    "‚ùóÔ∏è Again, remember that until 10 years ago, this problem was very challenging for the entire research community. As you have been sharpening your CNN skills, it is time to shine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rMQW1hVZhbT"
   },
   "source": [
    "## üõ† Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFWUX30r5wvo"
   },
   "source": [
    "If you are planning to use Google Colab (which is strongly recommended for this challenge), there is a few things you need to take care of first so that you can run everything properly.\n",
    "\n",
    "* If you want to get started, follow these four steps.\n",
    "* You can also read Davy's tutorial on Kitt üëâ [Introduction to Google Colab](https://kitt.lewagon.com/knowledge/tutorials/data_google_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6wVRDoPHetx"
   },
   "source": [
    "### Step 1: Upload the Challenge Folder to Google Drive and Open it in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSm6iw9HizE"
   },
   "source": [
    "If you want Google Colab to be able to run this notebook, you need to ***upload all the necessary files to your Google Drive***. \n",
    "\n",
    "To do this, simply:\n",
    "1. Access your [Google Drive](https://drive.google.com/)\n",
    "2. Go into the `Colab Notebooks folder`\n",
    "3. Drag-and-drop this `challenge's folder` into it\n",
    "4. Right-click the notebook file and select `Open with` $\\rightarrow$ `Google Colaboratory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur5KjRNA4cNR"
   },
   "source": [
    "### Step 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxIuwYAoG0TO"
   },
   "source": [
    "The previous was necessary but not enough. For security purposes, Google Colab and Google Drive are NOT connected even if they belong to the same Google Account\n",
    "\n",
    "We need to ***mount the main directory of the Google Drive associated with the account being used for Colab***.\n",
    "\n",
    "*Note: if you're getting errors when authenticating, try to do it in Google Chrome, as other browsers tend to experience issues.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16022,
     "status": "ok",
     "timestamp": 1660229898730,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "_q8uOTzh5vwJ",
    "outputId": "fe44eab3-4992-423d-f420-24341e2ef66e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tle_V6z6G9Mx"
   },
   "source": [
    "‚¨ÖÔ∏è On the left sidebar, there is a üóÇ folder icon. \n",
    "* If you click on it, you should now see a folder called `drive/MyDrive`. This is your Google Drive and all its content!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9tSxSyJIeK3"
   },
   "source": [
    "### Step 3: Navigate to the Challenge Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6v8XQ2IIjZp"
   },
   "source": [
    "One more thing: we need to  this notebook in the context of the challenge's folder. \n",
    "\n",
    "Simply run the cell down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1660229898730,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "7HCO04lsIrcd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.chdir allows you to change directories, like cd in the Terminal\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/data-cifar-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0_I4eSaJuNy"
   },
   "source": [
    "### Step 4: Toggle GPU Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4awrOZ6JxWs"
   },
   "source": [
    "As a last step, we should take advantage of Google Colab by enabling GPU acceleration for our notebook. \n",
    "\n",
    "You can do it by navigating through the menu bar:\n",
    "\n",
    "`Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator`\n",
    "\n",
    "and select \"GPU\" from the dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0MdAwhGJdSR"
   },
   "source": [
    "üöÄ You are now ready to start, proceed with the challenge! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY44N5e6T2Il"
   },
   "source": [
    "## (1) Loading the CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOWsFV-nZhba"
   },
   "source": [
    "‚ùì **Question: Loading the CIFAR10 Dataset** ‚ùì\n",
    "\n",
    "\n",
    "* üéÅ We took care of the `data loading and preprocessing` for you. \n",
    "* ‚ñ∂Ô∏è Just run the following cell and make sure you understand the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7846,
     "status": "ok",
     "timestamp": 1660229906573,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "ZkKdhZXWVj00",
    "outputId": "155099db-137d-4777-99c6-05e60ae43859"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(images_train, labels_train), (images_test, labels_test) = cifar10.load_data()\n",
    "\n",
    "labels = ['airplane', \n",
    "          'automobile',\n",
    "          'bird',\n",
    "          'cat',\n",
    "          'deer',\n",
    "          'dog',\n",
    "          'frog',\n",
    "          'horse',\n",
    "          'ship',\n",
    "          'truck']\n",
    "\n",
    "print(images_train.shape, images_test.shape)\n",
    "unique, counts = np.unique(labels_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18zIIeggZhbb"
   },
   "source": [
    "### (1.1) Working on a smaller dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI_3K-MHT2In"
   },
   "source": [
    "‚ùì **Question about the training size** ‚ùì\n",
    "\n",
    "* It will probably take a very long time to train a model on $50 000$ images m...\n",
    "* üë®üèª‚Äçüè´ **Always start with a subsample to iterate quickly** before scaling up! üÜô\n",
    "* Run the next cell where we are reducing the dataset size by `reduction_factor = 10`. Don't try to increase it unless we ask you to do so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660229906573,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "wR_JXHmlT2Io",
    "outputId": "113cc420-7b1c-4c57-c17a-f11b4681a945"
   },
   "outputs": [],
   "source": [
    "# Considering only 1/10th of the 50_000 images\n",
    "reduction_factor = 10\n",
    "\n",
    "# Choosing the random indices of small train set and small test set\n",
    "idx_train =  np.random.choice(len(images_train), round(len(images_train)/reduction_factor), replace=False)\n",
    "idx_test =  np.random.choice(len(images_test), round(len(images_test)/reduction_factor), replace=False)\n",
    "\n",
    "# Collecting the two subsamples images_train_small and images_test_small from images_train and images_test\n",
    "images_train_small = images_train[idx_train]\n",
    "images_test_small = images_test[idx_test]\n",
    "# and their corresponding labels\n",
    "labels_train_small = labels_train[idx_train]\n",
    "labels_test_small = labels_test[idx_test]\n",
    "\n",
    "print(\"------------------ Before -----------------\")\n",
    "print(images_train.shape, images_test.shape)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"--- After applying the reduction factor ---\")\n",
    "print(images_train_small.shape, images_test_small.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(\"-\"*43)\n",
    "\n",
    "unique, counts = np.unique(labels_train_small, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79M-HvdyZhbc"
   },
   "source": [
    "üëá You are working with images.. so it would be a good idea to have a look at some of them :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1660229907040,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "hQ62jK5VVj03",
    "outputId": "6a8438f8-d7e5-42b9-cd88-fa708e2ac8a4"
   },
   "outputs": [],
   "source": [
    "# Let's plot few images to see what they look like\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in range(6):\n",
    "    plt.subplot(1,6, i+1)\n",
    "    img = images_train[i]\n",
    "    label = labels_train[i][0]\n",
    "    plt.imshow(img)\n",
    "    plt.title(labels[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qlCaB_TZhbc"
   },
   "source": [
    "### (1.2) Image preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMG4ttoHVj02"
   },
   "source": [
    "üëâ As usual, let's:\n",
    "- normalize the pixels' intensities between 0 and 1\n",
    "- turn the `labels_train` and `labels_test` into \"one-hot-encoded\" targets that we will call respectively `y_train` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1660229907732,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "Fm8XRTbiVj02"
   },
   "outputs": [],
   "source": [
    "### Normalizing pixels' intensities\n",
    "X_train = images_train / 255.\n",
    "X_train_small = images_train_small / 255.\n",
    "X_test = images_test / 255.\n",
    "X_test_small = images_test_small / 255.\n",
    "\n",
    "### Encoding the labels\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(labels_train, 10)\n",
    "y_train_small = to_categorical(labels_train_small, 10)\n",
    "y_test = to_categorical(labels_test, 10)\n",
    "y_test_small = to_categorical(labels_test_small, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W9Jbjn6Zhbc"
   },
   "source": [
    "## (2) Iterate on your CNN architecture using your small training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke9YJk7MVj04"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì Time to shine ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è !\n",
    "\n",
    "1. Define the CNN architecture of your choice in an `initialize_model()` function:\n",
    "2. Compile your model in a `compile_model()` method:\n",
    "3. Fit your CNN only on the `small training set` and save the training information in an `history` variable\n",
    "---\n",
    "* Feeling lost?\n",
    "* Do you want to improve your performance ? \n",
    "\n",
    "<details>\n",
    "    <summary>üÜò PRO TIPS üÜò</summary>\n",
    "\n",
    "\n",
    "- Do not forget to add the **`input shape`** of your images to the first layer: it has 3 colors\n",
    "- **`Start simple, add complexify later `** after several attempts to get better results\n",
    "- The task is complex: **`Try at least 3 or 4 convolutions`**, you want your **images to go through different magnifying glasses / kernels from different convolutional layers!**\n",
    "- The Kernel Size does not need to be large for such a small picture resolution!\n",
    "- Add some **`MaxPooling2D`** (but not too much, otherwise the activation \"image\" will become too small)\n",
    "- Keep `padding = \"same\"` and `stride = (1,1)` to start with.\n",
    "- Once your model overfits, try to add some **`Dropout Layers` to regularize the network**. A good tip is to **increase the Dropout Rate/Strength as you move closer to the prediction layer** to prevent your CNN from overfitting\n",
    "- Images are so small in CIFAR10, you can afford to use a larger batch size (32 or 64) to benefit even more from **GPU parallelization**!\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1660229907733,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "GNOtE5xhVj04"
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    '''instanciate and return the CNN architecture of your choice with less than 150,000 params'''\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1660229907733,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "7C9AL4uBT2Iq"
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    '''return a compiled model suited for the CIFAR-10 task'''\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcPQtVjdVj06"
   },
   "source": [
    "‚ùì **Question: History of your training** ‚ùì \n",
    "\n",
    "Run the following function on the previous history \n",
    "_(keep the default arguments, these are intended for future plots in the notebook)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10196,
     "status": "ok",
     "timestamp": 1660229952607,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "lIR_L7UfVj06"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label = 'train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label = 'val' + exp_name)\n",
    "    ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1660229953048,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "bDxbLDt2Vj07",
    "outputId": "a213b9e1-eed3-45de-8840-00c9800055ac",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR7l9LxMVj07"
   },
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "* Evaluate your model on the test data and compare it with a baseline accuracy. \n",
    "* Are you satisfied with this performance?\n",
    "* Look at the `PRO TIPS` above and iterate a bit if you want to improve your performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1660229953254,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "WwnwvmV5Vj08",
    "outputId": "5a60d70d-3e07-4e60-820a-36470256e388",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7P9y-LwVjdx"
   },
   "source": [
    "## (3) Increase the size of your training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMHH_qsbVT_0"
   },
   "source": [
    "‚ùì **Question: train your model on the full dataset** ‚ùì \n",
    "\n",
    "- Switch to **Colab** if you haven't done it before\n",
    "- Make sure to use the **GPU acceleration** by clicking on `Runtime` $\\rightarrow$ `Change runtime` $\\rightarrow$ `GPU`\n",
    "\n",
    "üí° Training neural networks on images (in each batch) can be parallelized, and this **`parallelization procedure`** can be done on **`GPU`**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFMcHMwfZhbh"
   },
   "source": [
    "You should observe significant performance improvement\n",
    "\n",
    "Welcome to the Deep Learning paradigm, where big data makes a significant difference.\n",
    "\n",
    "But what happens if I can access only a limited amount of pictures? Think about biologists studying rare species. What can they do?\n",
    "* To improve the accuracy of a model without much work, we can **generate new data**. \n",
    "* The process is called... ‚≠êÔ∏è **`Data Augmentation`** ‚≠êÔ∏è ! \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIBg-oEkZhbh"
   },
   "source": [
    "## (4) üéÅüìö Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ÑπÔ∏è This section contains questions and the answers are also provided. Read them carefully before moving on to the `Transfer Learning` challenge.\n",
    "\n",
    "> üë®üèª‚Äçüè´ If you do not have time to do the `Transfer Learning` challenge, don't worry, we can talk about it during the Recap session, we are aware that this unit about Convolutional Neural Networks is quite packed... but also one of the most exciting applications of Deep Learning ‚ù§Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmPinqUzZhbh"
   },
   "source": [
    "* üë©üèª‚Äçüè´ <b><u>Data Augmentation</u></b>\n",
    "    * This technique is widely-used and consists of applying little transformations to the input images without changing their labels: ***mirroring***, ***cropping***, ***intensity changes***, etc... \n",
    "    * The _improved performance_ simply results from the CNN training with more images (the original pictures + the \"augmented\" ones).\n",
    "    \n",
    "\n",
    "* üëâ <b><u>Theoretically:</u></b>\n",
    "    * (1) We could generate these new images by applying some transformations on copies of the original pictures\n",
    "    * (2) Train the model on the original images + new images. \n",
    "    \n",
    "    \n",
    "* üö®  <b><u>Problem:</u></b>\n",
    "    * Such a procedure requires storing all these images in memory...\n",
    "    * It can be very intensive, so much so that your computer's RAM cannot hold them all\n",
    "    \n",
    "    \n",
    "* ü¶Ñ <b><u>In Practice:</u></b>\n",
    "    * When a Neural Network operates a forward/backward propagation, it requires to see only 16 pictures at a time if you chose $ batch size = 16 $ for example. It doesn't need to store all the original images or the augmented images in the RAM.\n",
    "    * For this reason, we will **augment the data on the fly (batch per batch)**. What does that mean? For every epoch and every batch, during the ***.fit()*** training procedure, we will:\n",
    "        1. Generate some `augmented data/images`\n",
    "        2. Fit the model on the images and their augmented versions\n",
    "        3. Delete the images and their augmented versions from the RAM\n",
    "        4. Repeat steps 1-2-3\n",
    "        \n",
    "* üìö <a href= \"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\"><b><u>tf/keras/preprocessing/image/ImageDataGenerator</u></b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk8ZZVunVj08"
   },
   "source": [
    "‚ùì **Question: using an ImageDataGenerator** ‚ùì\n",
    "\n",
    "Look at the following code down below üëá\n",
    "* The general syntax may look strange but don't worry: \n",
    "    * First, focus on the arguments of the *ImageDataGenerator* which define the augmentation techniques that we are using\n",
    "    * Then, check the üìö <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\">**`ImageDataGenerator`**</a> documentation later\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "aborted",
     "timestamp": 1660229998898,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "DlOpbos5Vj09"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center = False,\n",
    "    featurewise_std_normalization = False,\n",
    "    rotation_range = 10,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    horizontal_flip = True,\n",
    "    zoom_range = (0.8, 1.2),\n",
    "    ) \n",
    "\n",
    "datagen.fit(X_train)\n",
    "datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "aborted",
     "timestamp": 1660229998898,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "Zcl17dW1aA0G"
   },
   "outputs": [],
   "source": [
    "X_augmented_iterator = datagen.flow(X_train, shuffle=False, batch_size=1)\n",
    "X_augmented_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdMqg8grVj09"
   },
   "source": [
    "‚ùóÔ∏è Always **visualize the augmented images** in order to double-check whether you can still recognize the labels yourself or not ‚ùóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "aborted",
     "timestamp": 1660229998898,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "IsqMF0adVj09"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i, (raw_image, augmented_image) in enumerate(zip(X_train, X_augmented_iterator)):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 2))\n",
    "    ax1.imshow(raw_image)\n",
    "    ax2.imshow(augmented_image[0])\n",
    "    plt.show()\n",
    "    \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyLlJQrKVj09"
   },
   "source": [
    "‚ùó **Remarks** ‚ùó \n",
    "\n",
    "* Each image from **`X_augmented_iterator`** is an ***augmented image*** of one image located in the original `X_train` image dataset\n",
    "* This augmentation process is done once per epoch.\n",
    "* During one epoch, the model will:\n",
    "    1. *create the augmented version* of each picture from `X_train`, \n",
    "    2. for each image of `X_train`, *the model will randomly pick either the original version in `X_train` or its augmented version in `X_augmented_iterator`*\n",
    "    3. and the model will be *fitted on the combination of some original images + some augmented images*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCKCFU-KZhbj"
   },
   "source": [
    "‚ùì **Question: what is the validation set when we have augmented images** ‚ùì \n",
    "\n",
    "* Previously, we used the `validation_split` argument to let the model separate the training set into a Train/Validation split when fitting the model for each epoch.\n",
    "* It is not possible to use this kind of Train/Val Split here as **using an image in the training set and its transformation in the validation set is considered `data leakage`** !. \n",
    "* Therefore, we have to define the **`validation_data`** manually with the following commands: take time to understand the cell down below:üëá \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "aborted",
     "timestamp": 1660229998899,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "lYOkY7LOVj09"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# The model\n",
    "model_aug = initialize_model()\n",
    "model_aug = compile_model(model_aug)\n",
    "\n",
    "# The data generator\n",
    "X_tr = X_train[:40000]\n",
    "y_tr = y_train[:40000]\n",
    "X_val = X_train[40000:]\n",
    "y_val = y_train[40000:]\n",
    "train_flow = datagen.flow(X_tr, y_tr, batch_size = 64)\n",
    "\n",
    "# The early stopping criterion\n",
    "es = EarlyStopping(patience = 3)\n",
    "\n",
    "# The fit\n",
    "history_aug = model_aug.fit(train_flow, \n",
    "                        epochs = 50, \n",
    "                        callbacks = [es], \n",
    "                        validation_data = (X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmQ9t_cMVj0-"
   },
   "source": [
    "üö® The training can be quite long here...\n",
    "\n",
    "üëâ Feel free to move on to the next exercise and come back to this notebook later to finish the last questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSLb7gWRZhbj"
   },
   "source": [
    "‚ùì **Question: How did the model with an augmented dataset perform?** ‚ùì \n",
    "\n",
    "Let's plot the previous and current run histories. What do you think of the data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "aborted",
     "timestamp": 1660229998899,
     "user": {
      "displayName": "Bruno Castro Brunckhorst",
      "userId": "00660085536154297213"
     },
     "user_tz": -120
    },
    "id": "o9r4Fau2Vj0-",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm055RyNVj0-"
   },
   "source": [
    "ü•° <b><u>Some takeaways from Data Augmentation:</u></b>\n",
    "\n",
    "* Data augmentation may not improve your performance easily...\n",
    "\n",
    "* Here it even decreased the performance!\n",
    "\n",
    "* Its impact strongly depends on: \n",
    "    * the model architecture you used\n",
    "    * the learning rate, \n",
    "    * the type of augmentation chosen, etc...\n",
    "\n",
    "* Image classification is an art that requires months and years of practice to master!\n",
    "\n",
    "üö® **Don't spend too much time trying to finetune your model for the moment!  You have other interesting challenges to investigate!** üö®\n",
    "\n",
    "üìö [Here is a good example of a solution for future reference](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/).<br>\n",
    "They managed to reach an accuracy level of approx. 80%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oy4DCYLGVj0-"
   },
   "source": [
    "---\n",
    "\n",
    "üèÅ **Congratulations** üèÅ \n",
    "\n",
    "1. Download this notebook from your `Google Drive` or directly from `Google Colab` \n",
    "2. Drag-and-drop it from your `Downloads` folder to your local challenge folder  \n",
    "\n",
    "\n",
    "üíæ Don't forget to push your code\n",
    "\n",
    "3. Follow the usual procedure on your terminal, inside the challenge folder:\n",
    "      * *git add cifar_classification.ipynb*\n",
    "      * *git commit -m \"I am the god of CNNs\"*\n",
    "      * *git push origin master*\n",
    "\n",
    "*Hint*: To find where this Colab notebook has been saved, click on `File` $\\rightarrow$ `Locate in Drive`.\n",
    "\n",
    "üöÄ It is time to move on to the **Transfer Learning** challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
